# Web Scraper z Interfejsem Webowym

Aplikacja do pobierania stron internetowych wraz z obrazkami i zasobami, z moÅ¼liwoÅ›ciÄ… filtrowania HTML/JS oraz exportu do ZIP/PDF.

## ğŸš€ Quick Start

### Lokalne uruchomienie

```bash
# Klonuj repozytorium
git clone https://github.com/robmoteka/SCRAPPER.git
cd SCRAPPER

# Zbuduj aplikacjÄ™
go build -o scrapper ./cmd/server

# Uruchom serwer
./scrapper

# OtwÃ³rz w przeglÄ…darce
# http://localhost:8080
```

### Docker

```bash
# Build i uruchomienie
docker-compose up --build

# DostÄ™p do UI
# http://localhost:8080

# Zatrzymanie
docker-compose down
```

## âœ¨ Funkcje

- âœ… Scraping stron z kontrolÄ… gÅ‚Ä™bokoÅ›ci (1-5 poziomÃ³w)
- âœ… Pobieranie wszystkich zasobÃ³w (obrazki, CSS, JS)
- âœ… Transformacja linkÃ³w na Å›cieÅ¼ki wzglÄ™dne (offline-ready)
- âœ… Filtrowanie HTML/JS (usuwanie skryptÃ³w, reklam, etc.)
- âœ… Export do ZIP (peÅ‚na struktura projektu)
- âœ… Export do PDF (konsolidacja wszystkich stron w jeden dokument)
- âœ… Progress tracking w czasie rzeczywistym
- âœ… Prosty, responsywny interfejs webowy

## ğŸ“– Jak uÅ¼ywaÄ‡

### Przez interfejs webowy

1. OtwÃ³rz http://localhost:8080 w przeglÄ…darce
2. Wpisz URL strony do pobrania (np. `https://example.com`)
3. Ustaw gÅ‚Ä™bokoÅ›Ä‡ crawlingu (1-5):
   - **1** = tylko gÅ‚Ã³wna strona
   - **2** = gÅ‚Ã³wna strona + bezpoÅ›rednie linki
   - **3-5** = gÅ‚Ä™bsze poziomy
4. (Opcjonalnie) Dodaj filtry HTML/JS:
   ```
   <script|||</script>
   <!-- ads-start|||ads-end -->
   <div id="tracking"|||</div>
   ```
5. Kliknij **"Start Scraping"**
6. Obserwuj postÄ™p w czasie rzeczywistym
7. Po zakoÅ„czeniu:
   - **Pobierz ZIP** - kompletny offline-ready archiwum
   - **Generuj PDF** - wszystkie strony w jednym dokumencie

### Przez API

#### Rozpocznij scraping
```bash
curl -X POST http://localhost:8080/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "depth": 2,
    "filters": [
      {"start": "<script", "end": "</script>"},
      {"start": "<!-- ads", "end": "ads -->"}
    ]
  }'
```

OdpowiedÅº:
```json
{
  "project_id": "abc123-uuid",
  "status": "started"
}
```

#### SprawdÅº status
```bash
curl http://localhost:8080/api/project/{project_id}/status
```

#### Pobierz ZIP
```bash
curl http://localhost:8080/api/project/{project_id}/export/zip -o project.zip
```

#### Generuj PDF
```bash
curl -X POST http://localhost:8080/api/project/{project_id}/export/pdf -o project.pdf
```

## Stack Technologiczny

### Backend (Go 1.21+)
- `github.com/gocolly/colly/v2` - web scraping engine z kontrolÄ… gÅ‚Ä™bokoÅ›ci
- `github.com/PuerkitoBio/goquery` - parsing i modyfikacja HTML
- `github.com/go-chi/chi/v5` - routing HTTP (lekki, idiomatyczny)
- `github.com/jung-kurt/gofpdf` - generowanie PDF
- Standard library: `archive/zip`, `net/http`, `html`

### Frontend
- Vanilla HTML5/CSS3/JavaScript
- Fetch API do komunikacji z backendem
- Responsywny formularz + progress indicator

### Infrastruktura
- Docker multi-stage build (build + runtime)
- File system storage (bez bazy danych)
- Port 8080 (konfigurowalny przez ENV)

## Struktura Projektu

```
/scrapper
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ server/
â”‚       â””â”€â”€ main.go                    # Entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ handlers.go                # HTTP handlers
â”‚   â”‚   â””â”€â”€ routes.go                  # Routing setup
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ scraper.go                 # Colly scraping logic
â”‚   â”‚   â”œâ”€â”€ processor.go               # Link transformation
â”‚   â”‚   â””â”€â”€ filter.go                  # HTML/JS filtering
â”‚   â”œâ”€â”€ export/
â”‚   â”‚   â”œâ”€â”€ zip.go                     # ZIP export
â”‚   â”‚   â””â”€â”€ pdf.go                     # PDF generation
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ types.go                   # Data structures
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html                     # UI formularz
â”‚   â”œâ”€â”€ style.css                      # Styling
â”‚   â””â”€â”€ app.js                         # Frontend logic
â”œâ”€â”€ data/                              # Runtime projects storage
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ README.md
```

### Struktura Danych

Zapisane projekty organizowane sÄ… w folderach:

```
data/
â””â”€â”€ {project-id}/
    â”œâ”€â”€ index.html                     # GÅ‚Ã³wna strona
    â”œâ”€â”€ filters.json                   # ReguÅ‚y filtrowania
    â”œâ”€â”€ assets/                        # Obrazki, CSS, JS
    â”‚   â”œâ”€â”€ css/
    â”‚   â”œâ”€â”€ js/
    â”‚   â””â”€â”€ img/
    â””â”€â”€ pages/                         # Podstrony
        â”œâ”€â”€ page1.html
        â””â”€â”€ page2.html
```

## FunkcjonalnoÅ›ci

### Scraping
- **Parametr gÅ‚Ä™bokoÅ›ci**: kontrola rekurencyjnego pobierania stron (1-5 poziomÃ³w)
- **Pobieranie zasobÃ³w**: obrazki, CSS, JS, fonty
- **Transformacja linkÃ³w**: konwersja na Å›cieÅ¼ki wzglÄ™dne dla przenoÅ›noÅ›ci
- **Crawling wewnÄ…trz domeny**: podÄ…Å¼anie za linkami tylko w obrÄ™bie tej samej domeny

### Filtrowanie HTML/JS
- **Wzorce tekstowe**: usuwanie fragmentÃ³w kodu miÄ™dzy "start pattern" a "end pattern"
- **Multiple rules**: moÅ¼liwoÅ›Ä‡ zastosowania wielu filtrÃ³w naraz
- **PrzykÅ‚ady**:
  - `<script|||</script>` - usuwa wszystkie skrypty
  - `<div id="ads"|||</div>` - usuwa div z reklamami
  - `<!-- comment-start|||comment-end -->` - usuwa komentarze

### Export
- **ZIP**: pakowanie caÅ‚ego projektu (HTML + assets) do archiwum
- **PDF**: konsolidacja wszystkich stron w jeden dokument PDF

### Interfejs Webowy
- Minimalistyczny formularz z polami:
  - URL strony do pobrania
  - GÅ‚Ä™bokoÅ›Ä‡ crawlingu (1-5)
  - Filtry HTML/JS (format: `START|||END`, kaÅ¼dy filtr w nowej linii)
- Progress indicator podczas scrapingu
- Przyciski do exportu ZIP i PDF po zakoÅ„czeniu

## âš™ï¸ Konfiguracja

### Environment Variables
- `PORT` - port serwera (default: 8080)
- `MAX_DEPTH_LIMIT` - maksymalna gÅ‚Ä™bokoÅ›Ä‡ (default: 5)
- `DATA_DIR` - katalog na dane (default: ./data)
- `TIMEOUT` - timeout dla requestÃ³w w sekundach (default: 30)
- `USER_AGENT` - custom User-Agent string (default: WebScraper/1.0)

## ğŸ” Testowanie

### Flow testowy:
1. OtwÃ³rz `http://localhost:8080`
2. WprowadÅº URL testowy (np. `https://example.com`)
3. Ustaw gÅ‚Ä™bokoÅ›Ä‡ na 2
4. Opcjonalnie dodaj filtry:
   ```
   <script|||</script>
   <style|||</style>
   ```
5. Kliknij "Start Scraping"
6. Obserwuj progress
7. Po zakoÅ„czeniu:
   - SprawdÅº pliki w `data/{project-id}/`
   - Zweryfikuj transformacjÄ™ linkÃ³w w HTML
   - Pobierz ZIP
   - Wygeneruj PDF

### Edge Cases:
- âœ… ZewnÄ™trzne linki (pozostawiÄ‡ absolutne)
- âœ… Duplikaty URLi (cache odwiedzonych)
- âœ… Assets z innych domen (pobraÄ‡ lub oznaczyÄ‡)
- âœ… Timeout dla wolnych stron
- âœ… Strony z bÅ‚Ä™dami 404/500
- âœ… Cykliczne odnoÅ›niki (infinite loops)

## ğŸ—ï¸ Architektura

### Decisions Technologiczne

- **Go zamiast Python**: WydajnoÅ›Ä‡, Å‚atwa konteneryzacja (single binary), native concurrency
- **Colly**: Mature library z built-in depth control, lepsze od raw HTTP clienta
- **Vanilla JS**: Minimalny UI nie wymaga frameworka, mniej dependencies
- **File system storage**: Bez bazy danych, prostsze dla portable deployments
- **Struktura projektu**: Jeden projekt = jeden folder z assets i pages dla czytelnoÅ›ci
- **PDF format**: Konsolidacja wszystkich stron w jeden dokument
- **Filters JSON**: Osobny plik dla Å‚atwej edycji przed/po scrapingu
- **Relative links**: PrzenoÅ›noÅ›Ä‡ - scrapowane strony dziaÅ‚ajÄ… offline bez serwera

## ğŸ“ Known Limitations

### Current Scope (v1.0)
- Static HTML only (no JavaScript rendering)
- Same-domain crawling only
- No authentication support
- No rate limiting
- No robots.txt compliance check

## Licencja

MIT

## Autor

Created: 17 lutego 2026
