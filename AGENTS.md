# AGENT.md - Kontekst dla AI Assistants

## Project Overview
Web Scraper z interfejsem webowym w jÄ™zyku Go do pobierania stron internetowych wraz z zasobami, filtrowaniem HTML/JS oraz exportem do ZIP/PDF.

**Data utworzenia**: 17 lutego 2026  
**Status**: âœ… Production Ready - All Agents Completed

---

## User Decisions (z ask_questions)

### PodjÄ™te Decyzje Technologiczne

1. **Backend Framework**: **Go** (z ekosystemem Colly/Chi)
   - UÅ¼ytkownik wybraÅ‚ Go zamiast Python lub Node.js
   - Uzasadnienie: wydajnoÅ›Ä‡, single binary, Å‚atwa konteneryzacja

2. **Frontend**: **Vanilla HTML/CSS/JavaScript**
   - Bez frameworkÃ³w (React/Vue odrzucone)
   - Minimalistyczny interfejs wystarczy dla use case

3. **Baza Danych**: **Brak** (tylko file system)
   - Wybrano storage oparty o pliki
   - Bez SQLite/PostgreSQL
   - Projekty zapisywane jako foldery w `data/`

4. **JavaScript Rendering**: **Nie wymagane**
   - Strony docelowe nie wymagajÄ… headless browsera
   - Wystarczy HTTP client + HTML parser (Colly + goquery)
   - Puppeteer/Playwright niepotrzebne

5. **UI Complexity**: **Minimalny**
   - Formularz + progress indicator + status
   - Bez rozbudowanej historii czy podglÄ…du w iframe

6. **Struktura Danych**: **Projekt â†’ Foldery**
   - Format: `data/{project-id}/{index.html, assets/, pages/}`
   - Odrzucono pÅ‚askÄ… strukturÄ™

7. **Dodatkowe Funkcje** (user request):
   - âœ… Export do ZIP
   - âœ… Export do PDF jako **jeden dokument** (wszystkie strony skonsolidowane)
   - âœ… Edycja filtrÃ³w przed/po scrapingu
   - âŒ PodglÄ…d pobranych stron w UI (nie wybrany)
   - âŒ Lista/zarzÄ…dzanie projektami (nie wybrany)

---

## Stack Technologiczny (zatwierdzone)

### Backend
- **Go 1.21+**
- `github.com/gocolly/colly/v2` - web scraping z depth control
- `github.com/PuerkitoBio/goquery` - parsing/modyfikacja HTML
- `github.com/go-chi/chi/v5` - HTTP routing (lekki, idiomatyczny)
- `github.com/jung-kurt/gofpdf` - generowanie PDF
- Standard library: `archive/zip`, `net/http`, `html`

### Frontend
- HTML5/CSS3/JavaScript (vanilla)
- Fetch API
- Responsive design

### Infrastructure
- Docker multi-stage build
- Port 8080 (configurable via ENV)
- Volume mount dla `/app/data`

---

## Kluczowe Wymagania Funkcjonalne

### 1. Scraping Engine
- **Input**: URL + gÅ‚Ä™bokoÅ›Ä‡ (1-5) + filtry
- **Output**: Folder projektu z HTML + assets
- **Behavior**:
  - Rekurencyjne crawlowanie do zadanej gÅ‚Ä™bokoÅ›ci
  - Tylko linki wewnÄ…trz tej samej domeny
  - Pobieranie obrazkÃ³w, CSS, JS, fontÃ³w
  - Cache odwiedzonych URLi (é˜²æ­¢ duplicates)
  - Timeout handling

### 2. Link Transformation (krytyczne!)
- **Cel**: PrzenoÅ›noÅ›Ä‡ - strony dziaÅ‚ajÄ… offline bez serwera
- **Mechanizm**: Konwersja absolutnych URLi na Å›cieÅ¼ki wzglÄ™dne
  - `https://example.com/page.html` â†’ `pages/page.html`
  - `https://example.com/img/logo.png` â†’ `assets/img/logo.png`
- **Atrybuty do transformacji**: `src`, `href`, `srcset`, `data-src`
- **WyjÄ…tek**: ZewnÄ™trzne linki (inna domena) pozostajÄ… absolutne

### 3. HTML/JS Filtering
- **Format filtrÃ³w**: Pseudo-regex z wzorcami "start" i "end"
  - PrzykÅ‚ad: `<script|||</script>` usuwa wszystko miÄ™dzy (wÅ‚Ä…cznie z tagami)
- **Zastosowanie**: Po pobraniu, przed zapisem HTML
- **Storage**: `data/{project-id}/filters.json`
- **Multiple rules**: Kolejne zastosowania (nie jednoczesne)

### 4. Export Mechanisms

#### ZIP Export
- Rekurencyjne pakowanie caÅ‚ego folderu projektu
- Streaming dla duÅ¼ych archiwÃ³w
- Nazwa: `{project-id}.zip`

#### PDF Export (WAÅ»NE!)
- **Konsolidacja**: Wszystkie strony (index.html + pages/*) w **jeden PDF**
- **Struktura**: KaÅ¼da strona HTML = nowy rozdziaÅ‚/sekcja w PDF
- **Konwersja**: HTML â†’ plain text (strip tags, zachowanie struktury)
- **Opcjonalnie**: Inline images w PDF
- **User input**: "export do pdf jako 1 dokument na scrap"

### 5. Web UI (minimalny)

**Formularz**:
- Input: URL (required, validation)
- Input: GÅ‚Ä™bokoÅ›Ä‡ (number, 1-5, default: 2)
- Textarea: Filtry (kaÅ¼da linia: `START|||END`)
- Button: "Start Scraping"

**Progress**:
- Spinner + status text
- Opcjonalnie: progress percentage

**Actions po zakoÅ„czeniu**:
- Button: "Download ZIP"
- Button: "Generate PDF"

**Brak**:
- Lista projektÃ³w
- PodglÄ…d stron
- Historia operacji

---

## File Structure (obowiÄ…zkowa)

```
/scrapper
â”œâ”€â”€ cmd/server/main.go          # Entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ handlers.go         # HTTP handlers
â”‚   â”‚   â””â”€â”€ routes.go           # Chi routing
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ scraper.go          # Colly logic + depth control
â”‚   â”‚   â”œâ”€â”€ processor.go        # Link transformation
â”‚   â”‚   â””â”€â”€ filter.go           # HTML/JS filtering
â”‚   â”œâ”€â”€ export/
â”‚   â”‚   â”œâ”€â”€ zip.go              # ZIP creation
â”‚   â”‚   â””â”€â”€ pdf.go              # PDF generation (consolidated)
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ types.go            # Structs: ScrapeRequest, FilterRule, etc.
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html              # UI
â”‚   â”œâ”€â”€ style.css               # Styling
â”‚   â””â”€â”€ app.js                  # Frontend logic
â”œâ”€â”€ data/                       # Runtime storage (Git-ignored)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â”œâ”€â”€ README.md
â””â”€â”€ AGENT.md                    # Ten plik
```

### Data Structure na dysku

```
data/
â””â”€â”€ {project-id}/               # UUID lub timestamp
    â”œâ”€â”€ index.html              # GÅ‚Ã³wna strona (transformed links)
    â”œâ”€â”€ filters.json            # FilterRule[] JSON
    â”œâ”€â”€ assets/
    â”‚   â”œâ”€â”€ css/
    â”‚   â”œâ”€â”€ js/
    â”‚   â””â”€â”€ img/
    â””â”€â”€ pages/
        â”œâ”€â”€ about.html          # Podstrona 1 (transformed)
        â””â”€â”€ contact.html        # Podstrona 2 (transformed)
```

---

## API Specification

### `POST /api/scrape`
**Request**:
```json
{
  "url": "https://example.com",
  "depth": 2,
  "filters": [
    {"start": "<script", "end": "</script>"},
    {"start": "<!-- ads", "end": "ads -->"}
  ]
}
```
**Response**:
```json
{
  "project_id": "abc123-uuid",
  "status": "started"
}
```

### `GET /api/project/{id}/status`
**Response**:
```json
{
  "status": "in_progress|completed|failed",
  "progress": 75,
  "pages_downloaded": 15,
  "total_pages": 20,
  "current_url": "https://example.com/page3",
  "errors": ["Failed to download: https://..."]
}
```

### `GET /api/project/{id}/export/zip`
- Download ready ZIP file
- Content-Type: `application/zip`
- Content-Disposition: `attachment; filename="{project-id}.zip"`

### `POST /api/project/{id}/export/pdf`
- Generate PDF on-demand (moÅ¼e trwaÄ‡)
- Return PDF file
- Content-Type: `application/pdf`
- Content-Disposition: `attachment; filename="{project-id}.pdf"`

---

## Implementation Order (priorytet)

### Phase 1: Foundation
1. Initialize Go module + dependencies
2. Create folder structure
3. Implement `models/types.go` (structs)
4. Setup Chi router + basic endpoints

### Phase 2: Core Scraping
5. Implement `scraper/scraper.go` (Colly integration)
   - MaxDepth configuration
   - OnHTML callbacks dla `a`, `img`, `link`, `script`
   - Domain filtering (same domain only)
   - Asset downloading
6. Implement `scraper/processor.go` (link transformation)
   - URL parsing
   - Relative path conversion
   - HTML attribute updates via goquery

### Phase 3: Filtering & Storage
7. Implement `scraper/filter.go`
   - FilterRule struct
   - ApplyFilters function (sequential pattern matching)
   - JSON persistence
8. File storage logic (create project folders)

### Phase 4: API Handlers
9. Implement `api/handlers.go`
   - ScrapeHandler (POST /api/scrape)
   - StatusHandler (GET /api/project/{id}/status)
   - Implement async scraping (goroutine + status tracking)

### Phase 5: Export Features
10. Implement `export/zip.go` (recursive archiving)
11. Implement `export/pdf.go` (HTML consolidation + gofpdf)
12. Export API handlers

### Phase 6: Web UI
13. Create `web/index.html` (form)
14. Create `web/style.css` (minimal responsive)
15. Create `web/app.js` (Fetch API + polling)

### Phase 7: Containerization
16. Write Dockerfile (multi-stage)
17. Write docker-compose.yml
18. Test in Docker environment

### Phase 8: Testing & Polish
19. Edge case handling (timeouts, 404s, cycles)
20. Error messages
21. Logging
22. Documentation updates

---

## Critical Implementation Notes

### Link Transformation Algorithm
```
FOR each HTML file:
  Parse with goquery
  FOR each element with href/src/srcset:
    url = resolve(attribute_value, base_url)
    IF url.domain == base_domain:
      relative_path = convert_to_project_relative(url)
      update_attribute(element, relative_path)
    ELSE:
      keep absolute (external link)
  Save modified HTML
```

### Filter Application Algorithm
```
FOR each FilterRule in filters:
  html = original_html
  WHILE true:
    start_pos = find(html, rule.start)
    IF start_pos == -1: BREAK
    end_pos = find(html[start_pos:], rule.end)
    IF end_pos == -1: BREAK
    html = html[:start_pos] + html[start_pos+end_pos+len(rule.end):]
  original_html = html
RETURN original_html
```

### PDF Consolidation Flow
```
1. List all HTML files in project (index.html + pages/*)
2. FOR each file:
     - Read HTML
     - Strip HTML tags â†’ plain text
     - Add section header (filename)
     - Add to PDF as new page/chapter
3. Optionally: Extract <img> src â†’ download â†’ embed in PDF
4. Generate final PDF file
5. Stream to client
```

---

## Environment Variables (to implement)

- `PORT` - Server port (default: 8080)
- `MAX_DEPTH_LIMIT` - Hard cap for depth param (default: 5)
- `DATA_DIR` - Storage path (default: ./data)
- `TIMEOUT` - HTTP request timeout in seconds (default: 30)
- `USER_AGENT` - Custom UA string (default: "WebScraper/1.0")

---

## Testing Checklist

### Functional Tests
- [ ] Scraping depth=1: tylko gÅ‚Ã³wna strona
- [ ] Scraping depth=2: gÅ‚Ã³wna + bezpoÅ›rednie linki
- [ ] Link transformation: absolutne â†’ wzglÄ™dne
- [ ] External links: pozostajÄ… absolutne
- [ ] Assets downloading: images, CSS, JS
- [ ] Filter application: `<script|||</script>` usuwa skrypty
- [ ] Multiple filters: kolejne zastosowania
- [ ] ZIP export: peÅ‚na struktura projektu
- [ ] PDF export: konsolidacja wszystkich stron
- [ ] Progress tracking: status updates podczas scrapingu

### Edge Cases
- [ ] Duplicate URLs: cache dziaÅ‚a
- [ ] External assets: obsÅ‚uga cross-domain
- [ ] 404/500 errors: graceful degradation
- [ ] Timeouts: nie blokuje caÅ‚ej operacji
- [ ] Circular links: nie powoduje infinite loop
- [ ] Large sites: memory management

### Docker Tests
- [ ] Build succeeds
- [ ] Container runs na porcie 8080
- [ ] Volume persistence: data/ survives restart
- [ ] ENV variables: konfiguracja dziaÅ‚a

---

## Known Limitations & Future Work

### Current Scope (v1.0)
- Static HTML only (no JS rendering)
- Same-domain crawling only
- No authentication support
- No rate limiting
- No robots.txt check

### Potential Extensions (out of scope now)
- Headless browser dla JS-heavy sites (Playwright)
- Multi-domain crawling
- Authentication (basic auth, cookies)
- Cron scheduling
- UI: project management, preview
- Metrics & analytics
- Robots.txt compliance

---

## Development Commands

```bash
# Initialize
go mod init github.com/user/scrapper
go get github.com/gocolly/colly/v2
go get github.com/PuerkitoBio/goquery
go get github.com/go-chi/chi/v5
go get github.com/jung-kurt/gofpdf

# Run locally
go run cmd/server/main.go

# Build
go build -o scrapper cmd/server/main.go

# Docker
docker-compose up --build
docker-compose down

# Test
go test ./...
```

---

## AI Assistant Guidelines

When implementing this project:

1. **Follow the structure exactly** - folder layout is designed for Go best practices
2. **Implement in phases** - don't skip foundation work
3. **Test incrementally** - verify each phase before moving on
4. **Respect user decisions** - they explicitly rejected certain features (database, complex UI, JS rendering)
5. **Prioritize core features** - scraping + link transformation + filtering are critical
6. **Keep UI minimal** - user wants simplicity, not feature creep
7. **Focus on portability** - relative links are essential for offline use
8. **Consolidate PDF** - user wants ONE document per scrape, not multiple files
9. **Handle errors gracefully** - web scraping is inherently fragile
10. **Document as you go** - inline comments for complex logic (especially filters & transformations)

### When in doubt:
- Check this AGENT.md for user decisions
- Refer to README.md for technical details
- Ask clarifying questions rather than assume
- Default to simpler solution (MVP mindset)

---

**Last Updated**: 17 lutego 2026  
**Ready for Implementation**: âœ… Yes

---

## âœ… Implementation Complete

**Completion Date**: 22 lutego 2026  
**All Agents**: Fully Implemented  
**Deployment Status**: Docker-ready

### Final Verification

- âœ… All 8 agents completed
- âœ… 22/22 tasks implemented
- âœ… E2E tests passing
- âœ… Docker deployment functional
- âœ… Documentation up-to-date

### Production Readiness Checklist

- âœ… Core functionality (scraping, filtering, transformation)
- âœ… API layer (REST endpoints, async, status tracking)
- âœ… Export features (ZIP, PDF consolidated)
- âœ… Web UI (responsive, real-time updates)
- âœ… Containerization (Docker, docker-compose)
- âœ… Error handling (graceful degradation)
- âœ… Logging (structured, levels)
- âœ… Documentation (README, AGENTS, per-agent guides)

**Ready for Deployment** ğŸš€
